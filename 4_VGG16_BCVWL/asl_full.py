# -*- coding: utf-8 -*-
"""ASL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zQAyT0JScEqWg3NAfKaZnv7D_-MZyuaF

**ASL**
"""
# !ls "/content/drive/My Drive/"

# !python3 "/content/drive/My Drive/ASL_Colab/asl_preprocess_data.py"

"""**Pre Processing Code**"""

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

import os
import cv2
import random
import numpy as np
import keras
from random import shuffle
from keras.utils import np_utils
from shutil import unpack_archive

print("Imported Modules...")

# --------------------------------------------------------------------------
# Folowing is to unzip the whole data and load into the Drive

# # unpack_archive('foo.zip', 'destination_path/')
#unpack_archive('data.zip', 'asl_data')
class_dic = {"L":0,"B":1,"C":2,"V":3,"W":4}

print("Unpacked Dataset")

image_list = []
image_class = []

path = "../asl_alphabet_data"


data_folder_path = "../asl_alphabet_data"


files = os.listdir(data_folder_path)

print(type(files))

required_files = []

symbols = ['B','C','L','V','W']

for i in files:
	if i[0] in symbols:
		required_files.append(i)


for i in range(10):
	shuffle(required_files)

print("Shuffled Data Files")
  


# print(type(files))
print(len(required_files))

class_count = {'L':0,'B':0,'C':0,'V':0,'W':0}

X = []
Y = []
X_val = []
Y_val = []
X_test = []
Y_test = []
unique_list=[]

for file_name in required_files:
#  print(file_name)
  label = file_name[0]
  if label not in unique_list:
    print(label)
    unique_list.append(label)
  path = data_folder_path+'/'+file_name
  image = cv2.imread(path)
  resized_image = cv2.resize(image,(224,224))
  if class_count[label]<2700:
    class_count[label]+=1
    X.append(resized_image)
    Y.append(class_dic[label])
  #elif class_count[label]>=2000 and class_count[label]<2750:
  #  class_count[label]+=1
  #  X_val.append(resized_image)
  #  Y_val.append(class_dic[label])
  else:
    X_test.append(resized_image)
    Y_test.append(class_dic[label])

print(len(unique_list))
	
Y = np_utils.to_categorical(Y)
#Y_val = np_utils.to_categorical(Y_val)
Y_test = np_utils.to_categorical(Y_test)

print(len(Y))
#print(len(Y_val))
print(len(Y_test))

print(len(X))
#print(len(X_val))
print(len(X_test))

npy_data_path  = "Numpy"

np.save(npy_data_path+'/train_set.npy',X)
np.save(npy_data_path+'/train_classes.npy',Y)

#np.save(npy_data_path+'/validation_set.npy',X_val)
#np.save(npy_data_path+'/validation_classes.npy',Y_val)

np.save(npy_data_path+'/test_set.npy',X_test)
np.save(npy_data_path+'/test_classes.npy',Y_test)

print("Data pre-processing Success!")

#get train and validation sets
# npy_data_path  = "/content/drive/My Drive/ASL_Colab/Image_To_Numpy_Data"

X_train=np.load(npy_data_path+"/train_set.npy")
Y_train=np.load(npy_data_path+"/train_classes.npy")

#X_valid=np.load(npy_data_path+"/validation_set.npy")
#Y_valid=np.load(npy_data_path+"/validation_classes.npy")

X_test=np.load(npy_data_path+"/test_set.npy")
Y_test=np.load(npy_data_path+"/test_classes.npy")

X_test.shape

import keras
from keras.optimizers import SGD
from keras.models import Sequential
from keras.applications import VGG16
from keras.preprocessing import image
from keras.layers.normalization import BatchNormalization
from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D

print("Imported Network Essentials")

#Load the VGG model
image_size=224
vgg_base = VGG16(weights='imagenet',include_top=False,input_shape=(image_size,image_size,3))

#initiate a model
model = Sequential()

#Add the VGG base model
model.add(vgg_base)

#Add new layers
model.add(Flatten())

model.add(Dense(8192, activation='relu'))
model.add(Dropout(0.8))
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(5, activation='softmax'))



# (4) Compile 
sgd = SGD(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
checkpoint = keras.callbacks.ModelCheckpoint("Weights/weights.{epoch:02d}-{val_loss:.2f}.hdf5", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)
# (5) Train
model.fit(X_train/255.0, Y_train, batch_size=32, epochs=15, verbose=1,validation_data=(X_test/255.0,Y_test/255.0), shuffle=True,callbacks=[checkpoint])

# serialize model to JSON
model_json = model.to_json()
with open("Model/model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("Model/model_weights.h5")
print("Saved model to disk")


