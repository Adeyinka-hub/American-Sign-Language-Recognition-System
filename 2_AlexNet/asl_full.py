# -*- coding: utf-8 -*-
"""ASL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zQAyT0JScEqWg3NAfKaZnv7D_-MZyuaF

**ASL**
"""
# !ls "/content/drive/My Drive/"

# !python3 "/content/drive/My Drive/ASL_Colab/asl_preprocess_data.py"

"""**Pre Processing Code**"""

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

import os
import cv2
import random
import numpy as np
import keras
from random import shuffle
from keras.utils import np_utils
from shutil import unpack_archive

print("Imported Modules...")

# --------------------------------------------------------------------------
# Folowing is to unzip the whole data and load into the Drive

# # unpack_archive('foo.zip', 'destination_path/')
#unpack_archive('data.zip', 'asl_data')
class_dic = {"A":0,"B":1,"C":2}

print("Unpacked Dataset")

image_list = []
image_class = []

path = "asl_data/new"


data_folder_path = "asl_data/new"

files = os.listdir(data_folder_path)

for i in range(10):
	shuffle(files)

print("Shuffled Data Files")
  


# print(type(files))
print(len(files))

class_count = {'A':0,'B':0,'C':0}
#class_count= {'A':0,'B':0,'C':0,"D":0,"E":0,"F":0,"G":0,"H":0,"I":0,"K":0,
#              "L":0,"M":0,"N":0,"O":0,"P":0,"Q":0,"R":0,"S":0,"T":0,
#             "U":0,"V":0,"W":0,"X":0,"Y":0}

X = []
Y = []
X_val = []
Y_val = []
X_test = []
Y_test = []
unique_list=[]

for file_name in files:
#  print(file_name)
  label = file_name[0]
  if label not in unique_list:
    print(label)
    unique_list.append(label)
  path = data_folder_path+'/'+file_name
  image = cv2.imread(path)
  resized_image = cv2.resize(image,(224,224))
  if class_count[label]<2000:
    class_count[label]+=1
    X.append(resized_image)
    Y.append(class_dic[label])
  elif class_count[label]>=2000 and class_count[label]<2750:
    class_count[label]+=1
    X_val.append(resized_image)
    Y_val.append(class_dic[label])
  else:
    X_test.append(resized_image)
    Y_test.append(class_dic[label])

print(len(unique_list))
	
Y = np_utils.to_categorical(Y)
Y_val = np_utils.to_categorical(Y_val)
Y_test = np_utils.to_categorical(Y_test)

print(len(Y))
print(len(Y_val))
print(len(Y_test))

print(len(X))
print(len(X_val))
print(len(X_test))

npy_data_path  = "Numpy_Full"

np.save(npy_data_path+'/train_set.npy',X)
np.save(npy_data_path+'/train_classes.npy',Y)

np.save(npy_data_path+'/validation_set.npy',X_val)
np.save(npy_data_path+'/validation_classes.npy',Y_val)

np.save(npy_data_path+'/test_set.npy',X_test)
np.save(npy_data_path+'/test_classes.npy',Y_test)

print("Data pre-processing Success!")

#get train and validation sets
# npy_data_path  = "/content/drive/My Drive/ASL_Colab/Image_To_Numpy_Data"

X_train=np.load(npy_data_path+"/train_set.npy")
Y_train=np.load(npy_data_path+"/train_classes.npy")

X_valid=np.load(npy_data_path+"/validation_set.npy")
Y_valid=np.load(npy_data_path+"/validation_classes.npy")

X_test=np.load(npy_data_path+"/test_set.npy")
Y_test=np.load(npy_data_path+"/test_classes.npy")

X_test.shape

from keras.optimizers import SGD
from keras.models import Sequential
from keras.preprocessing import image
from keras.layers.normalization import BatchNormalization
from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D

print("Imported Network Essentials")

model = Sequential()
# 1st Convolutional Layer
model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),strides=(4,4), padding='valid'))
model.add(Activation('relu'))
# Pooling 
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
# Batch Normalisation before passing it to the next layer
model.add(BatchNormalization())

# 2nd Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Pooling
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
# Batch Normalisation
model.add(BatchNormalization())

# 3rd Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Batch Normalisation
model.add(BatchNormalization())

# 4th Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Batch Normalisation
model.add(BatchNormalization())

# 5th Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))
model.add(Activation('relu'))
# Pooling
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
# Batch Normalisation
model.add(BatchNormalization())

# Passing it to a dense layer
model.add(Flatten())
# 1st Dense Layer
model.add(Dense(8192, input_shape=(224*224*3,)))
model.add(Activation('relu'))
# Add Dropout to prevent overfitting
model.add(Dropout(0.7))
# Batch Normalisation
model.add(BatchNormalization())

# 2nd Dense Layer
model.add(Dense(8192))
model.add(Activation('relu'))
# Add Dropout
model.add(Dropout(0.7))
# Batch Normalisation
model.add(BatchNormalization())

# 3rd Dense Layer
model.add(Dense(4096))
model.add(Activation('relu'))
# Add Dropout
model.add(Dropout(0.6))
# Batch Normalisation
model.add(BatchNormalization())

# Output Layer
model.add(Dense(3))
model.add(Activation('softmax'))

model.summary()

# (4) Compile 
sgd = SGD(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
checkpoint = keras.callbacks.ModelCheckpoint("Checkpoint/weights.{epoch:02d}-{val_loss:.2f}.hdf5", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)
# (5) Train
model.fit(X_train/255.0, Y_train, batch_size=32, epochs=512, verbose=1,validation_data=(X_valid/255.0,Y_valid/255.0), shuffle=True,callbacks=[checkpoint])

# serialize model to JSON
model_json = model.to_json()
with open("Weights_Full/model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("Weights_Full/model_weights.h5")
print("Saved model to disk")


